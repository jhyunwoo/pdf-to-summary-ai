# VESSL í™˜ê²½ ì„¤ì¹˜ ë° ì„¤ì • ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” VESSL í™˜ê²½ì—ì„œ Ollamaì™€ Qwen3-VL:235b ëª¨ë¸ì„ ì„¤ì •í•˜ëŠ” ë°©ë²•ì„ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [VESSL í™˜ê²½ ì´í•´í•˜ê¸°](#vessl-í™˜ê²½-ì´í•´í•˜ê¸°)
2. [ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­](#ë¦¬ì†ŒìŠ¤-ìš”êµ¬ì‚¬í•­)
3. [ë‹¨ê³„ë³„ ì„¤ì¹˜ ê°€ì´ë“œ](#ë‹¨ê³„ë³„-ì„¤ì¹˜-ê°€ì´ë“œ)
4. [VESSL Run ì„¤ì •](#vessl-run-ì„¤ì •)
5. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

## ğŸŒ VESSL í™˜ê²½ ì´í•´í•˜ê¸°

VESSLì€ ë¨¸ì‹ ëŸ¬ë‹ ì›Œí¬í”Œë¡œìš°ë¥¼ ìœ„í•œ í´ë¼ìš°ë“œ í”Œë«í¼ì…ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤:

- **ì˜êµ¬ ì €ì¥ì†Œ**: `/workspace` ë””ë ‰í† ë¦¬ê°€ ì˜êµ¬ ë³¼ë¥¨ìœ¼ë¡œ ë§ˆìš´íŠ¸ë¨
- **GPU ì ‘ê·¼**: NVIDIA GPUë¥¼ ì§ì ‘ ì‚¬ìš© ê°€ëŠ¥
- **í¬íŠ¸ í¬ì›Œë”©**: ì›¹ ì„œë¹„ìŠ¤ë¥¼ ì™¸ë¶€ì— ë…¸ì¶œ ê°€ëŠ¥
- **í™˜ê²½ ë³€ìˆ˜**: ì„¤ì •ì„ í†µí•´ í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬

## ğŸ’» ë¦¬ì†ŒìŠ¤ ìš”êµ¬ì‚¬í•­

### ìµœì†Œ ìš”êµ¬ì‚¬í•­

```yaml
resources:
  cluster: vessl-gcp-oregon
  preset: gpu-l-mem  # ìµœì†Œ 48GB VRAM
```

### ê¶Œì¥ ì‚¬ì–‘

```yaml
resources:
  cluster: vessl-gcp-oregon
  preset: gpu-xl-mem  # A100 80GB ë˜ëŠ” H100
```

### ë””ìŠ¤í¬ ê³µê°„

- **ëª¨ë¸ í¬ê¸°**: ì•½ 235GB (qwen3-vl:235b)
- **ì‹œìŠ¤í…œ ë° ê¸°íƒ€**: ì•½ 50GB
- **ê¶Œì¥ ì´ ìš©ëŸ‰**: ìµœì†Œ 500GB

## ğŸš€ ë‹¨ê³„ë³„ ì„¤ì¹˜ ê°€ì´ë“œ

### ë°©ë²• 1: VESSL Web UI ì‚¬ìš©

#### 1ë‹¨ê³„: í”„ë¡œì íŠ¸ ìƒì„±

1. VESSL ì›¹ì‚¬ì´íŠ¸ (https://vessl.ai) ë¡œê·¸ì¸
2. ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±
3. í”„ë¡œì íŠ¸ ì´ë¦„: `ollama-qwen3vl-server`

#### 2ë‹¨ê³„: Run ìƒì„±

1. "Create Run" í´ë¦­
2. ë‹¤ìŒ ì •ë³´ ì…ë ¥:
   - **Name**: `ollama-qwen3vl-api-server`
   - **Image**: `quay.io/vessl-ai/torch:2.0.1-cuda11.8-r15`
   - **Cluster**: ì‚¬ìš© ê°€ëŠ¥í•œ GPU í´ëŸ¬ìŠ¤í„° ì„ íƒ
   - **Preset**: `gpu-l-mem` ë˜ëŠ” `gpu-xl-mem`

#### 3ë‹¨ê³„: ì½”ë“œ Import ì„¤ì •

**Git Repository ì‚¬ìš©**:
```yaml
import:
  /code:
    git:
      url: https://github.com/yourusername/pdf-to-summary-ai.git
      ref: main
```

**ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ì—…ë¡œë“œ**:
- ZIP íŒŒì¼ë¡œ ì••ì¶•í•˜ì—¬ ì—…ë¡œë“œ

#### 4ë‹¨ê³„: ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ì„¤ì •

```bash
#!/bin/bash
set -e

cd /code

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ìŠ¤í¬ë¦½íŠ¸ ê¶Œí•œ ë¶€ì—¬
chmod +x *.sh

# Ollama ì„¤ì¹˜
./setup_ollama.sh

# Ollama ì„œë²„ ì‹œì‘
./start_ollama.sh

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì„ íƒ: ì‚¬ì „ì— ë‹¤ìš´ë¡œë“œëœ ê²½ìš° ì œì™¸)
echo "y" | ./download_model.sh

# API ì„œë²„ ì‹œì‘
python server.py
```

#### 5ë‹¨ê³„: í¬íŠ¸ ì„¤ì •

- Port 8000 (API ì„œë²„): HTTPë¡œ ë…¸ì¶œ
- Port 11434 (Ollama): ë‚´ë¶€ í†µì‹ ìš©

#### 6ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```yaml
env:
  OLLAMA_HOST: http://localhost:11434
  MODEL_NAME: qwen3-vl:235b
  PORT: 8000
  HOST: 0.0.0.0
  OLLAMA_MODELS: /workspace/.ollama/models
```

#### 7ë‹¨ê³„: ë³¼ë¥¨ ë§ˆìš´íŠ¸ ì„¤ì •

```yaml
mount:
  /workspace:
    volume:
      name: ollama-models
      size: 500Gi
```

### ë°©ë²• 2: VESSL CLI ì‚¬ìš©

#### 1ë‹¨ê³„: VESSL CLI ì„¤ì¹˜

```bash
pip install vessl
vessl configure
```

ì„¤ì • ì‹œ í•„ìš”í•œ ì •ë³´:
- VESSL API Token (ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°œê¸‰)
- Organization name
- Project name

#### 2ë‹¨ê³„: vessl.yaml íŒŒì¼ ì‚¬ìš©

í”„ë¡œì íŠ¸ì— í¬í•¨ëœ `vessl.yaml` íŒŒì¼ì„ ì‚¬ìš©:

```bash
# Run ìƒì„±
vessl run create -f vessl.yaml

# ë˜ëŠ” ì»¤ìŠ¤í…€ ì„¤ì •ìœ¼ë¡œ ìƒì„±
vessl run create \
  --image quay.io/vessl-ai/torch:2.0.1-cuda11.8-r15 \
  --cluster vessl-gcp-oregon \
  --preset gpu-l-mem \
  --git-ref main \
  --volume ollama-models:/workspace:500Gi
```

#### 3ë‹¨ê³„: Run ìƒíƒœ í™•ì¸

```bash
# Run ëª©ë¡ ë³´ê¸°
vessl run list

# íŠ¹ë³„ Run ìƒíƒœ í™•ì¸
vessl run read <run-number>

# ë¡œê·¸ í™•ì¸
vessl run logs <run-number>
```

#### 4ë‹¨ê³„: í¬íŠ¸ í¬ì›Œë”©

```bash
# ë¡œì»¬ ë¨¸ì‹ ì—ì„œ VESSL Runì— ì ‘ì†
vessl run port-forward <run-number> 8000:8000
```

ê·¸ í›„ ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:8000/docs` ì ‘ì†

## ğŸ“ VESSL Run ì„¤ì • íŒŒì¼ ìƒì„¸

### ì „ì²´ vessl.yaml ì˜ˆì œ

```yaml
name: ollama-qwen3vl-server
description: Ollama Qwen3-VL Vision Language Model API Server

# ë² ì´ìŠ¤ ì´ë¯¸ì§€
image: quay.io/vessl-ai/torch:2.0.1-cuda11.8-r15

# ë¦¬ì†ŒìŠ¤ í• ë‹¹
resources:
  cluster: vessl-gcp-oregon
  preset: gpu-xl-mem  # A100 80GB ê¶Œì¥

# ì½”ë“œ ê°€ì ¸ì˜¤ê¸°
import:
  /code:
    git:
      url: https://github.com/yourusername/pdf-to-summary-ai.git
      ref: main

# ì‹¤í–‰ ëª…ë ¹ì–´
run:
  - command: |
      set -e
      cd /code
      
      # Python ì˜ì¡´ì„±
      pip install -r requirements.txt
      
      # ìŠ¤í¬ë¦½íŠ¸ ê¶Œí•œ
      chmod +x setup_ollama.sh start_ollama.sh download_model.sh
      
      # Ollama ì„¤ì¹˜
      ./setup_ollama.sh
      
      # Ollama ì„œë²„ ì‹œì‘
      ./start_ollama.sh
      sleep 10
      
      # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
      if ! ollama list | grep -q "qwen3-vl:235b"; then
        echo "y" | ./download_model.sh
      else
        echo "Model already exists"
      fi
      
      # API ì„œë²„ ì‹œì‘
      python server.py

# í¬íŠ¸ ë…¸ì¶œ
ports:
  - name: api
    type: http
    port: 8000
  - name: ollama
    type: http
    port: 11434

# ì‘ì—… ë””ë ‰í† ë¦¬
workdir: /code

# í™˜ê²½ ë³€ìˆ˜
env:
  OLLAMA_HOST: http://localhost:11434
  MODEL_NAME: qwen3-vl:235b
  PORT: 8000
  HOST: 0.0.0.0
  OLLAMA_MODELS: /workspace/.ollama/models
  PYTHONUNBUFFERED: "1"

# ì˜êµ¬ ë³¼ë¥¨
mount:
  /workspace:
    volume:
      name: ollama-models
      size: 500Gi

# ì¬ì‹œì‘ ì •ì±…
restart: on-failure
max_retries: 3
```

## ğŸ”§ ê³ ê¸‰ ì„¤ì •

### 1. ì‚¬ì „ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©

ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œê°„ì„ ì¤„ì´ë ¤ë©´:

1. **ë³„ë„ì˜ Runì—ì„œ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ**:

```bash
vessl run create \
  --image quay.io/vessl-ai/torch:2.0.1-cuda11.8-r15 \
  --cluster vessl-gcp-oregon \
  --preset gpu-l-mem \
  --volume ollama-models:/workspace:500Gi \
  --command "curl -fsSL https://ollama.com/install.sh | sh && \
             ollama serve & sleep 10 && \
             ollama pull qwen3-vl:235b"
```

2. **ë™ì¼í•œ ë³¼ë¥¨ì„ ì¬ì‚¬ìš©**í•˜ì—¬ API ì„œë²„ Run ìƒì„±

### 2. ë©€í‹° GPU ì„¤ì •

ì—¬ëŸ¬ GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´:

```yaml
resources:
  cluster: vessl-gcp-oregon
  preset: gpu-xl-mem
  gpu_count: 2  # 2ê°œì˜ GPU ì‚¬ìš©

env:
  CUDA_VISIBLE_DEVICES: "0,1"
```

### 3. ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§

**ë¡œê·¸ í™•ì¸**:
```bash
# ì‹¤ì‹œê°„ ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë°
vessl run logs <run-number> --follow

# íŠ¹ì • ì‹œê°„ëŒ€ ë¡œê·¸
vessl run logs <run-number> --since 1h
```

**ë©”íŠ¸ë¦­ í™•ì¸**:
- VESSL ì›¹ UIì—ì„œ GPU ì‚¬ìš©ë¥ , ë©”ëª¨ë¦¬ ë“± í™•ì¸
- Grafana ëŒ€ì‹œë³´ë“œ ì—°ë™ ê°€ëŠ¥

### 4. í™˜ê²½ ë³€ìˆ˜ë¡œ ë¯¼ê° ì •ë³´ ê´€ë¦¬

```bash
# Secret ìƒì„±
vessl secret create api-keys \
  --from-literal=API_KEY=your_api_key

# Runì—ì„œ ì‚¬ìš©
vessl run create \
  --secret api-keys \
  ...
```

## ğŸ” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œê°„ ì´ˆê³¼

**ì¦ìƒ**: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘ ì—°ê²°ì´ ëŠê¹€

**í•´ê²°ì±…**:
```bash
# ë‹¤ìš´ë¡œë“œ ì¬ì‹œë„
ollama pull qwen3-vl:235b

# ë˜ëŠ” ë‹¤ìš´ë¡œë“œ íƒ€ì„ì•„ì›ƒ ì¦ê°€
export OLLAMA_DOWNLOAD_TIMEOUT=3600
ollama pull qwen3-vl:235b
```

### ë¬¸ì œ 2: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ**: `CUDA out of memory` ì˜¤ë¥˜

**í•´ê²°ì±…**:
1. ë” í° GPU í”„ë¦¬ì…‹ ì‚¬ìš© (A100 80GB ì´ìƒ)
2. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©: `qwen3-vl:14b`
3. ì–‘ìí™” ëª¨ë¸ ì‚¬ìš©: `qwen3-vl:235b-q4`

```bash
# ì–‘ìí™” ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull qwen3-vl:235b-q4
```

### ë¬¸ì œ 3: í¬íŠ¸ ì ‘ê·¼ ë¶ˆê°€

**ì¦ìƒ**: API ì„œë²„ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŒ

**í•´ê²°ì±…**:
```bash
# 1. Runì—ì„œ í¬íŠ¸ê°€ ì œëŒ€ë¡œ ë…¸ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸
vessl run read <run-number>

# 2. ë¡œì»¬ì—ì„œ í¬íŠ¸ í¬ì›Œë”©
vessl run port-forward <run-number> 8000:8000

# 3. ë°©í™”ë²½ ê·œì¹™ í™•ì¸
# VESSL ì›¹ UI > Run > Ports ì„¹ì…˜ í™•ì¸
```

### ë¬¸ì œ 4: ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±

**ì¦ìƒ**: "No space left on device" ì˜¤ë¥˜

**í•´ê²°ì±…**:
```bash
# 1. ë¶ˆí•„ìš”í•œ íŒŒì¼ ì‚­ì œ
rm -rf /tmp/*
docker system prune -a

# 2. ë³¼ë¥¨ í¬ê¸° ì¦ê°€
# vessl.yamlì—ì„œ volume size ì¦ê°€
mount:
  /workspace:
    volume:
      size: 1000Gi  # 1TBë¡œ ì¦ê°€
```

### ë¬¸ì œ 5: Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨

**ì¦ìƒ**: "Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"

**í•´ê²°ì±…**:
```bash
# 1. Ollama í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep ollama

# 2. Ollama ì¬ì‹œì‘
pkill ollama
./start_ollama.sh

# 3. ë¡œê·¸ í™•ì¸
cat ollama.log

# 4. ìˆ˜ë™ìœ¼ë¡œ Ollama ì‹œì‘
ollama serve &
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### 1. ëª¨ë¸ ì‚¬ì „ ë¡œë“œ

```python
# server.pyì— ì¶”ê°€
@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ì‚¬ì „ ë¡œë“œ"""
    try:
        # ë”ë¯¸ ìš”ì²­ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ
        requests.post(
            f"{OLLAMA_HOST}/api/generate",
            json={"model": MODEL_NAME, "prompt": "test", "stream": False}
        )
        print("âœ… Model preloaded successfully")
    except Exception as e:
        print(f"âš ï¸  Model preload failed: {e}")
```

### 2. ë™ì‹œ ìš”ì²­ ì²˜ë¦¬

```bash
# ì›Œì»¤ ìˆ˜ ì¦ê°€
uvicorn server:app --host 0.0.0.0 --port 8000 --workers 4
```

### 3. ìºì‹± í™œìš©

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def process_cached_request(prompt_hash):
    # ë™ì¼í•œ ìš”ì²­ì€ ìºì‹œì—ì„œ ë°˜í™˜
    pass
```

## ğŸ“š ì¶”ê°€ ìë£Œ

- [VESSL ê³µì‹ ë¬¸ì„œ](https://docs.vessl.ai/)
- [Ollama GitHub](https://github.com/ollama/ollama)
- [Qwen3-VL ëª¨ë¸ ì •ë³´](https://huggingface.co/Qwen)
- [FastAPI ê³µì‹ ë¬¸ì„œ](https://fastapi.tiangolo.com/)

## ğŸ’¡ ìœ ìš©í•œ ëª…ë ¹ì–´ ëª¨ìŒ

```bash
# VESSL Run ê´€ë¦¬
vessl run list                          # Run ëª©ë¡
vessl run read <run-number>             # Run ìƒì„¸ ì •ë³´
vessl run logs <run-number>             # ë¡œê·¸ í™•ì¸
vessl run terminate <run-number>        # Run ì¢…ë£Œ
vessl run port-forward <run-number> 8000:8000  # í¬íŠ¸ í¬ì›Œë”©

# Ollama ê´€ë¦¬
ollama list                             # ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡
ollama pull <model-name>                # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama rm <model-name>                  # ëª¨ë¸ ì‚­ì œ
ollama ps                               # ì‹¤í–‰ ì¤‘ì¸ ëª¨ë¸ í™•ì¸

# ì„œë²„ ê´€ë¦¬
ps aux | grep python                    # Python í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep ollama                    # Ollama í”„ë¡œì„¸ìŠ¤ í™•ì¸
netstat -tulpn | grep 8000              # í¬íŠ¸ ì‚¬ìš© í™•ì¸
curl http://localhost:8000/health       # í—¬ìŠ¤ì²´í¬
```

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

ì„¤ì¹˜ê°€ ì™„ë£Œë˜ë©´ ë‹¤ìŒ í•­ëª©ì„ í™•ì¸í•˜ì„¸ìš”:

- [ ] Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ê°€? (`ps aux | grep ollama`)
- [ ] ëª¨ë¸ì´ ë‹¤ìš´ë¡œë“œë˜ì—ˆëŠ”ê°€? (`ollama list`)
- [ ] API ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ê°€? (`curl http://localhost:8000/`)
- [ ] GPUê°€ ì¸ì‹ë˜ëŠ”ê°€? (`nvidia-smi`)
- [ ] í¬íŠ¸ê°€ ì—´ë ¤ìˆëŠ”ê°€? (`netstat -tulpn | grep 8000`)
- [ ] í—¬ìŠ¤ì²´í¬ê°€ ì •ìƒì¸ê°€? (`curl http://localhost:8000/health`)

ëª¨ë“  í•­ëª©ì´ ì²´í¬ë˜ì—ˆë‹¤ë©´ ì •ìƒì ìœ¼ë¡œ ì„¤ì¹˜ê°€ ì™„ë£Œëœ ê²ƒì…ë‹ˆë‹¤! ğŸ‰

