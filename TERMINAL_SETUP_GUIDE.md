# í„°ë¯¸ë„ ê¸°ë°˜ ì„¤ì¹˜ ë° ì‹¤í–‰ ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” Docker ì—†ì´ í„°ë¯¸ë„ì—ì„œ ì§ì ‘ ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ì—¬ Ollama Qwen3-VL ì„œë²„ë¥¼ ì„¤ì¹˜í•˜ê³  ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [ì „ì œ ì¡°ê±´](#ì „ì œ-ì¡°ê±´)
2. [ë‹¨ê³„ë³„ ì„¤ì¹˜ ê°€ì´ë“œ](#ë‹¨ê³„ë³„-ì„¤ì¹˜-ê°€ì´ë“œ)
3. [ì„œë²„ ì‹¤í–‰](#ì„œë²„-ì‹¤í–‰)
4. [í…ŒìŠ¤íŠ¸](#í…ŒìŠ¤íŠ¸)
5. [ì„œë²„ ì¢…ë£Œ](#ì„œë²„-ì¢…ë£Œ)
6. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

## ğŸ”§ ì „ì œ ì¡°ê±´

### í•„ìˆ˜ ì†Œí”„íŠ¸ì›¨ì–´
- **OS**: Linux (Ubuntu 20.04+) ë˜ëŠ” macOS
- **Python**: 3.9 ì´ìƒ
- **GPU**: NVIDIA GPU (ì„ íƒì‚¬í•­, í•˜ì§€ë§Œ ê°•ë ¥ ê¶Œì¥ - ìµœì†Œ 24GB VRAM)
- **CUDA**: 11.8+ (GPU ì‚¬ìš© ì‹œ)
- **ë””ìŠ¤í¬**: ìµœì†Œ 100GB ì—¬ìœ  ê³µê°„

### ì‹œìŠ¤í…œ í™•ì¸

```bash
# OS í™•ì¸
uname -a

# Python ë²„ì „ í™•ì¸
python3 --version

# GPU í™•ì¸ (NVIDIA GPUê°€ ìˆëŠ” ê²½ìš°)
nvidia-smi

# ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
df -h
```

## ğŸš€ ë‹¨ê³„ë³„ ì„¤ì¹˜ ê°€ì´ë“œ

### ë°©ë²• 1: ìë™ ì„¤ì¹˜ (ê¶Œì¥)

#### 1ë‹¨ê³„: í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™

```bash
cd /Users/jhyunwoo/projects/pdf-to-summary-ai
```

#### 2ë‹¨ê³„: ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬

```bash
chmod +x setup_venv.sh setup_ollama.sh start_ollama.sh download_model.sh quick_start.sh run_server.sh stop_all.sh
```

#### 3ë‹¨ê³„: ë¹ ë¥¸ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰

```bash
./quick_start.sh
```

ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ ìë™ìœ¼ë¡œ ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
- Python ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
- Python íŒ¨í‚¤ì§€ ì„¤ì¹˜
- Ollama ì„¤ì¹˜
- Ollama ì„œë²„ ì‹œì‘
- ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì„ íƒ)

#### 4ë‹¨ê³„: ì„œë²„ ì‹¤í–‰

```bash
./run_server.sh
```

### ë°©ë²• 2: ìˆ˜ë™ ì„¤ì¹˜ (ë‹¨ê³„ë³„)

#### 1ë‹¨ê³„: Python ê°€ìƒí™˜ê²½ ì„¤ì •

```bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd /Users/jhyunwoo/projects/pdf-to-summary-ai

# ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv venv

# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# pip ì—…ê·¸ë ˆì´ë“œ
pip install --upgrade pip setuptools wheel

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ì„¤ì¹˜ í™•ì¸
pip list
```

#### 2ë‹¨ê³„: Ollama ì„¤ì¹˜

```bash
# Ollama ì„¤ì¹˜
curl -fsSL https://ollama.com/install.sh | sh

# ì„¤ì¹˜ í™•ì¸
ollama --version
```

#### 3ë‹¨ê³„: Ollama ì„œë²„ ì‹œì‘

```bash
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export OLLAMA_HOST=0.0.0.0:11434
export OLLAMA_MODELS=/workspace/.ollama/models

# ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p /workspace/.ollama/models

# Ollama ì„œë²„ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
nohup ollama serve > ollama.log 2>&1 &

# ì„œë²„ ì‹œì‘ ëŒ€ê¸°
sleep 5

# ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:11434/api/tags
```

#### 4ë‹¨ê³„: Qwen3-VL ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì•½ 32GB)
ollama pull qwen3-vl:32b

# ë‹¤ìš´ë¡œë“œ í™•ì¸
ollama list
```

**ëŒ€ì•ˆ: ë‹¤ë¥¸ í¬ê¸°ì˜ ëª¨ë¸**
```bash
# ë” ì‘ì€ 14B ë²„ì „
ollama pull qwen3-vl:14b

# ì–‘ìí™” ë²„ì „ (VRAM ì ˆì•½)
ollama pull qwen3-vl:32b-q4

# ë” í° ëª¨ë¸ (ë” ë§ì€ ë¦¬ì†ŒìŠ¤ í•„ìš”)
ollama pull qwen3-vl:235b
```

#### 5ë‹¨ê³„: API ì„œë²„ ì‹œì‘

```bash
# ê°€ìƒí™˜ê²½ì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
source venv/bin/activate

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì„ íƒì‚¬í•­)
export OLLAMA_HOST=http://localhost:11434
export MODEL_NAME=qwen3-vl:32b
export PORT=8000
export HOST=0.0.0.0

# ì„œë²„ ì‹œì‘ (í¬ê·¸ë¼ìš´ë“œ)
python server.py
```

**ë˜ëŠ” ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰**:
```bash
nohup python server.py > server.log 2>&1 &

# PID í™•ì¸
echo $!

# ë¡œê·¸ í™•ì¸
tail -f server.log
```

## ğŸŒ ì„œë²„ ì‹¤í–‰

### ê°„ë‹¨í•œ ì‹¤í–‰

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# ì„œë²„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
./run_server.sh
```

### ìˆ˜ë™ ì‹¤í–‰

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# Ollama ì„œë²„ í™•ì¸
if ! pgrep -x "ollama" > /dev/null; then
    ./start_ollama.sh
fi

# API ì„œë²„ ì‹œì‘
python server.py
```

### í¬íŠ¸ ë³€ê²½

```bash
# ë‹¤ë¥¸ í¬íŠ¸ë¡œ ì‹¤í–‰
PORT=8080 python server.py
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

### 1. ì„œë²„ ìƒíƒœ í™•ì¸

```bash
# ê¸°ë³¸ ìƒíƒœ í™•ì¸
curl http://localhost:8000/

# í—¬ìŠ¤ì²´í¬
curl http://localhost:8000/health
```

### 2. í…ìŠ¤íŠ¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸

```bash
curl -X POST "http://localhost:8000/api/generate/text" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
    "temperature": 0.7,
    "max_tokens": 500
  }'
```

### 3. ì´ë¯¸ì§€ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸

```bash
# ì´ë¯¸ì§€ íŒŒì¼ ì¤€ë¹„
# test.jpg íŒŒì¼ì´ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ìˆë‹¤ê³  ê°€ì •

curl -X POST "http://localhost:8000/api/generate" \
  -F "image=@test.jpg" \
  -F "prompt=ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ìˆë‚˜ìš”?" \
  -F "temperature=0.7" \
  -F "max_tokens=1000"
```

### 4. Python í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python test_client.py

# ì´ë¯¸ì§€ì™€ í•¨ê»˜ í…ŒìŠ¤íŠ¸
python test_client.py test.jpg "ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"
```

### 5. ì˜ˆì œ ì‹¤í–‰

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# ì˜ˆì œ ì‹¤í–‰
python example_usage.py test.jpg
```

## ğŸ›‘ ì„œë²„ ì¢…ë£Œ

### ë°©ë²• 1: ìë™ ì¢…ë£Œ ìŠ¤í¬ë¦½íŠ¸

```bash
./stop_all.sh
```

### ë°©ë²• 2: ìˆ˜ë™ ì¢…ë£Œ

```bash
# API ì„œë²„ ì¢…ë£Œ
pkill -f "python server.py"

# Ollama ì„œë²„ ì¢…ë£Œ
pkill ollama

# í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep -E "ollama|python server.py"
```

### ë°©ë²• 3: í¬ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°

```bash
# Ctrl+C í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œë©ë‹ˆë‹¤
```

## ğŸ”„ ì¼ìƒì ì¸ ì‚¬ìš© ì›Œí¬í”Œë¡œìš°

### ì„œë²„ ì‹œì‘

```bash
cd /Users/jhyunwoo/projects/pdf-to-summary-ai
source venv/bin/activate
./run_server.sh
```

### ì„œë²„ ì¢…ë£Œ

```bash
./stop_all.sh
```

### ìƒíƒœ í™•ì¸

```bash
# Ollama ì„œë²„ í™•ì¸
ps aux | grep ollama

# API ì„œë²„ í™•ì¸
ps aux | grep "python server.py"

# í¬íŠ¸ í™•ì¸
netstat -tulpn | grep 8000
netstat -tulpn | grep 11434

# ë˜ëŠ” lsof ì‚¬ìš©
lsof -i :8000
lsof -i :11434
```

### ë¡œê·¸ í™•ì¸

```bash
# Ollama ë¡œê·¸
tail -f ollama.log

# API ì„œë²„ ë¡œê·¸ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì‹œ)
tail -f server.log
```

## ğŸ” ë¬¸ì œ í•´ê²°

### ë¬¸ì œ 1: ê°€ìƒí™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•ŠìŒ

**ì¦ìƒ**: `python` ëª…ë ¹ì–´ê°€ ì‹œìŠ¤í…œ Pythonì„ ê°€ë¦¬í‚´

**í•´ê²°ì±…**:
```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# í™•ì¸
which python
python --version
```

### ë¬¸ì œ 2: Ollama ì„œë²„ê°€ ì‹œì‘ë˜ì§€ ì•ŠìŒ

**ì¦ìƒ**: `Connection refused` ì˜¤ë¥˜

**í•´ê²°ì±…**:
```bash
# ë¡œê·¸ í™•ì¸
cat ollama.log

# Ollama í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep ollama

# ìˆ˜ë™ìœ¼ë¡œ ì‹œì‘
ollama serve > ollama.log 2>&1 &

# ì ì‹œ ëŒ€ê¸°
sleep 5

# ì—°ê²° í…ŒìŠ¤íŠ¸
curl http://localhost:11434/api/tags
```

### ë¬¸ì œ 3: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨

**ì¦ìƒ**: ë‹¤ìš´ë¡œë“œ ì¤‘ ì—°ê²° ëŠê¹€ ë˜ëŠ” ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±

**í•´ê²°ì±…**:
```bash
# ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
df -h

# ë¶ˆí•„ìš”í•œ íŒŒì¼ ì‚­ì œ
rm -rf /tmp/*

# ëª¨ë¸ ì¬ë‹¤ìš´ë¡œë“œ
ollama pull qwen3-vl:32b

# ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
ollama pull qwen3-vl:14b

# ì–‘ìí™” ë²„ì „ ì‚¬ìš© (ì ˆë°˜ í¬ê¸°)
ollama pull qwen3-vl:32b-q4
```

### ë¬¸ì œ 4: API ì„œë²„ í¬íŠ¸ê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘

**ì¦ìƒ**: `Address already in use` ì˜¤ë¥˜

**í•´ê²°ì±…**:
```bash
# í¬íŠ¸ ì‚¬ìš© í™•ì¸
lsof -i :8000

# í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
kill -9 <PID>

# ë˜ëŠ” ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
PORT=8080 python server.py
```

### ë¬¸ì œ 5: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ**: `CUDA out of memory` ì˜¤ë¥˜

**í•´ê²°ì±…**:
```bash
# GPU ì‚¬ìš©ëŸ‰ í™•ì¸
nvidia-smi

# ì–‘ìí™” ëª¨ë¸ ì‚¬ìš© (VRAM ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ)
export MODEL_NAME=qwen3-vl:32b-q4
python server.py

# ë˜ëŠ” ë” ì‘ì€ ëª¨ë¸
export MODEL_NAME=qwen3-vl:14b
python server.py
```

### ë¬¸ì œ 6: íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì˜¤ë¥˜

**ì¦ìƒ**: `pip install` ì¤‘ ì˜¤ë¥˜

**í•´ê²°ì±…**:
```bash
# pip ì—…ê·¸ë ˆì´ë“œ
pip install --upgrade pip

# ìºì‹œ í´ë¦¬ì–´
pip cache purge

# ì¬ì„¤ì¹˜
pip install -r requirements.txt

# íŠ¹ì • íŒ¨í‚¤ì§€ ë¬¸ì œ ì‹œ
pip install <package_name> --force-reinstall
```

## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### GPU ëª¨ë‹ˆí„°ë§

```bash
# ì‹¤ì‹œê°„ GPU ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# ë˜ëŠ”
nvidia-smi -l 1
```

### ì„œë²„ ë©”íŠ¸ë¦­

```bash
# CPU ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
top

# Python í”„ë¡œì„¸ìŠ¤ë§Œ
top -p $(pgrep -f "python server.py")

# ë„¤íŠ¸ì›Œí¬ ì—°ê²°
netstat -an | grep 8000
```

### API ì‘ë‹µ ì‹œê°„ ì¸¡ì •

```bash
# curlë¡œ ì‘ë‹µ ì‹œê°„ ì¸¡ì •
time curl -X POST "http://localhost:8000/api/generate/text" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello", "max_tokens": 100}'
```

## ğŸ” í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

### .env íŒŒì¼ ìƒì„±

```bash
# .env íŒŒì¼ ìƒì„±
cat > .env << EOF
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODELS=/workspace/.ollama/models
MODEL_NAME=qwen3-vl:32b
PORT=8000
HOST=0.0.0.0
PYTHONUNBUFFERED=1
EOF

# .env íŒŒì¼ ë¡œë“œ
source .env
```

### ì„¸ì…˜ë³„ í™˜ê²½ ë³€ìˆ˜

```bash
# í˜„ì¬ ì„¸ì…˜ì—ë§Œ ì ìš©
export OLLAMA_HOST=http://localhost:11434
export MODEL_NAME=qwen3-vl:32b
export PORT=8000

# ì˜êµ¬ì ìœ¼ë¡œ ì„¤ì • (bashrcì— ì¶”ê°€)
echo "export OLLAMA_HOST=http://localhost:11434" >> ~/.bashrc
echo "export MODEL_NAME=qwen3-vl:32b" >> ~/.bashrc
source ~/.bashrc
```

## ğŸ“ ìœ ìš©í•œ ëª…ë ¹ì–´ ëª¨ìŒ

```bash
# ì „ì²´ ì„¤ì¹˜ ë° ì‹œì‘
./quick_start.sh && ./run_server.sh

# ì„œë²„ë§Œ ì¬ì‹œì‘
./stop_all.sh && ./run_server.sh

# ë¡œê·¸ ì‹¤ì‹œê°„ í™•ì¸
tail -f ollama.log server.log

# ê°€ìƒí™˜ê²½ ì¬ìƒì„±
rm -rf venv && ./setup_venv.sh

# ëª¨ë¸ ëª©ë¡ í™•ì¸
ollama list

# ëª¨ë¸ ì‚­ì œ
ollama rm qwen3-vl:32b

# ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
du -sh .ollama/models/*
du -sh venv/

# í”„ë¡œì„¸ìŠ¤ íŠ¸ë¦¬ í™•ì¸
pstree -p | grep -E "ollama|python"

# ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
htop
```

## ğŸš€ í”„ë¡œë•ì…˜ ë°°í¬ íŒ

### systemd ì„œë¹„ìŠ¤ë¡œ ë“±ë¡

```bash
# systemd ì„œë¹„ìŠ¤ íŒŒì¼ ìƒì„±
sudo tee /etc/systemd/system/ollama-api.service << EOF
[Unit]
Description=Ollama Qwen3-VL API Server
After=network.target

[Service]
Type=simple
User=$USER
WorkingDirectory=/Users/jhyunwoo/projects/pdf-to-summary-ai
ExecStartPre=/bin/bash start_ollama.sh
ExecStart=/Users/jhyunwoo/projects/pdf-to-summary-ai/venv/bin/python server.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

# ì„œë¹„ìŠ¤ í™œì„±í™” ë° ì‹œì‘
sudo systemctl daemon-reload
sudo systemctl enable ollama-api
sudo systemctl start ollama-api

# ìƒíƒœ í™•ì¸
sudo systemctl status ollama-api
```

### ì—­ë°©í–¥ í”„ë¡ì‹œ (Nginx)

```bash
# Nginx ì„¤ì •
sudo tee /etc/nginx/sites-available/ollama-api << EOF
server {
    listen 80;
    server_name your-domain.com;

    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host \$host;
        proxy_set_header X-Real-IP \$remote_addr;
        proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
        proxy_read_timeout 300s;
    }
}
EOF

# ì„¤ì • í™œì„±í™”
sudo ln -s /etc/nginx/sites-available/ollama-api /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl reload nginx
```

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. **API ë¬¸ì„œ í™•ì¸**: http://localhost:8000/docs
2. **ì˜ˆì œ ì‹¤í–‰**: `python example_usage.py test.jpg`
3. **ì»¤ìŠ¤í…€ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ**: APIë¥¼ í™œìš©í•œ ì•± ê°œë°œ
4. **ì„±ëŠ¥ íŠœë‹**: ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¡°ì •

---

ëª¨ë“  ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ‰

