name: ollama-qwen3vl-server
description: Ollama Qwen3-VL Vision Language Model API Server

# Docker ì´ë¯¸ì§€ ì„¤ì •
image: quay.io/vessl-ai/torch:2.0.1-cuda11.8-r15

# ë¦¬ì†ŒìŠ¤ ì„¤ì • (A100 ë˜ëŠ” H100 ê¶Œì¥)
resources:
  cluster: vessl-gcp-oregon  # ì‚¬ìš© ê°€ëŠ¥í•œ í´ëŸ¬ìŠ¤í„°ë¡œ ë³€ê²½
  preset: gpu-l-mem  # ë˜ëŠ” A100/H100 í”„ë¦¬ì…‹

# ì½”ë“œ ê°€ì ¸ì˜¤ê¸°
import:
  /code:
    git:
      url: https://github.com/yourusername/pdf-to-summary-ai.git  # ì‹¤ì œ ì €ì¥ì†Œ URLë¡œ ë³€ê²½
      ref: main

# ì‹¤í–‰ ëª…ë ¹
run:
  - command: |
      set -e
      cd /code
      
      echo "ğŸ“¦ Python ì˜ì¡´ì„± ì„¤ì¹˜ ì¤‘..."
      pip install -r requirements.txt
      
      echo "ğŸ”§ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬..."
      chmod +x setup_ollama.sh start_ollama.sh download_model.sh
      
      echo "ğŸ“¥ Ollama ì„¤ì¹˜ ì¤‘..."
      ./setup_ollama.sh
      
      echo "ğŸš€ Ollama ì„œë²„ ì‹œì‘ ì¤‘..."
      ./start_ollama.sh
      
      echo "â³ Ollama ì„œë²„ ì•ˆì •í™” ëŒ€ê¸°..."
      sleep 10
      
      echo "ğŸ“¥ Qwen3-VL ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘..."
      echo "âš ï¸  ì´ ì‘ì—…ì€ ì¸í„°ë„· ì†ë„ì— ë”°ë¼ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤..."
      echo "y" | ./download_model.sh || {
        echo "ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ ì¤‘..."
        ollama pull qwen3-vl:32b
      }
      
      echo "âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!"
      ollama list
      
      echo "ğŸŒ API ì„œë²„ ì‹œì‘ ì¤‘..."
      python server.py

# í¬íŠ¸ ì„¤ì •
ports:
  - name: api
    type: http
    port: 3000
  - name: ollama
    type: http
    port: 11434

# ì‘ì—… ë””ë ‰í† ë¦¬
workdir: /code

# í™˜ê²½ ë³€ìˆ˜
env:
  OLLAMA_HOST: http://localhost:11434
  MODEL_NAME: qwen3-vl:32b
  PORT: 3000
  HOST: 0.0.0.0
  OLLAMA_MODELS: /workspace/.ollama/models

# ë³¼ë¥¨ ë§ˆìš´íŠ¸ (ëª¨ë¸ ì˜êµ¬ ì €ì¥)
mount:
  /workspace:
    volume:
      name: ollama-models
      size: 100Gi  # 32b ëª¨ë¸ì— ì¶©ë¶„í•œ í¬ê¸°

