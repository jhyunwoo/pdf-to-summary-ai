"""
Ollama Qwen3-VL API Server
ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ollamaì˜ qwen3-vl:235b ëª¨ë¸ë¡œ ì²˜ë¦¬í•˜ëŠ” ì„œë²„
"""
import os
import base64
from typing import Optional
from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import requests
import json
from io import BytesIO
from PIL import Image

app = FastAPI(
    title="Ollama Qwen3-VL API Server",
    description="ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” Vision Language Model ì„œë²„",
    version="1.0.0"
)

# Ollama API ì„¤ì •
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
MODEL_NAME = os.getenv("MODEL_NAME", "qwen3-vl:235b")


class TextPromptRequest(BaseModel):
    """í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬í•˜ëŠ” ìš”ì²­"""
    prompt: str
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 2000


def encode_image_to_base64(image_bytes: bytes) -> str:
    """ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©"""
    return base64.b64encode(image_bytes).decode('utf-8')


def validate_image(image_bytes: bytes) -> bool:
    """ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì¦"""
    try:
        img = Image.open(BytesIO(image_bytes))
        img.verify()
        return True
    except Exception:
        return False


@app.get("/")
async def root():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "running",
        "message": "Ollama Qwen3-VL API Server",
        "model": MODEL_NAME,
        "ollama_host": OLLAMA_HOST
    }


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ë° ollama ì—°ê²° í™•ì¸"""
    try:
        # Ollama ì„œë²„ ì—°ê²° í™•ì¸
        response = requests.get(f"{OLLAMA_HOST}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            model_exists = any(MODEL_NAME in model.get("name", "") for model in models)
            
            return {
                "status": "healthy",
                "ollama_connected": True,
                "model_loaded": model_exists,
                "available_models": [model.get("name") for model in models]
            }
        else:
            return JSONResponse(
                status_code=503,
                content={
                    "status": "unhealthy",
                    "ollama_connected": False,
                    "error": "Ollama server is not responding properly"
                }
            )
    except Exception as e:
        return JSONResponse(
            status_code=503,
            content={
                "status": "unhealthy",
                "ollama_connected": False,
                "error": str(e)
            }
        )


@app.post("/api/generate")
async def generate_with_image(
    image: UploadFile = File(..., description="ì…ë ¥ ì´ë¯¸ì§€ íŒŒì¼"),
    prompt: str = Form(..., description="ì²˜ë¦¬í•  í”„ë¡¬í”„íŠ¸"),
    temperature: float = Form(0.7, description="ìƒì„± ì˜¨ë„ (0.0-1.0)"),
    max_tokens: int = Form(2000, description="ìµœëŒ€ í† í° ìˆ˜")
):
    """
    ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì•„ì„œ Qwen3-VL ëª¨ë¸ë¡œ ì²˜ë¦¬
    
    Args:
        image: ì—…ë¡œë“œëœ ì´ë¯¸ì§€ íŒŒì¼
        prompt: ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        temperature: ìƒì„± ì˜¨ë„ (ë‚®ì„ìˆ˜ë¡ ê²°ì •ì , ë†’ì„ìˆ˜ë¡ ì°½ì˜ì )
        max_tokens: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜
    
    Returns:
        ëª¨ë¸ì˜ ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    try:
        # ì´ë¯¸ì§€ ì½ê¸°
        image_bytes = await image.read()
        
        # ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì¦
        if not validate_image(image_bytes):
            raise HTTPException(status_code=400, detail="ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€ í˜•ì‹ì…ë‹ˆë‹¤.")
        
        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
        image_base64 = encode_image_to_base64(image_bytes)
        
        # Ollama API ìš”ì²­ ì¤€ë¹„
        payload = {
            "model": MODEL_NAME,
            "prompt": prompt,
            "images": [image_base64],
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens
            }
        }
        
        # Ollama API í˜¸ì¶œ
        response = requests.post(
            f"{OLLAMA_HOST}/api/generate",
            json=payload,
            timeout=300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ (í° ëª¨ë¸ì´ë¯€ë¡œ)
        )
        
        if response.status_code != 200:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"Ollama API ì˜¤ë¥˜: {response.text}"
            )
        
        result = response.json()
        
        return {
            "success": True,
            "response": result.get("response", ""),
            "model": MODEL_NAME,
            "prompt": prompt,
            "done": result.get("done", False),
            "context": result.get("context", []),
            "total_duration": result.get("total_duration"),
            "load_duration": result.get("load_duration"),
            "prompt_eval_count": result.get("prompt_eval_count"),
            "eval_count": result.get("eval_count")
        }
        
    except requests.exceptions.Timeout:
        raise HTTPException(status_code=504, detail="Ollama ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
    except requests.exceptions.ConnectionError:
        raise HTTPException(status_code=503, detail="Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")


@app.post("/api/generate/text")
async def generate_text_only(request: TextPromptRequest):
    """
    í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬ (ì´ë¯¸ì§€ ì—†ì´)
    
    Args:
        request: í”„ë¡¬í”„íŠ¸ì™€ ìƒì„± ì˜µì…˜ì„ í¬í•¨í•œ ìš”ì²­
    
    Returns:
        ëª¨ë¸ì˜ ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    try:
        # Ollama API ìš”ì²­ ì¤€ë¹„
        payload = {
            "model": MODEL_NAME,
            "prompt": request.prompt,
            "stream": False,
            "options": {
                "temperature": request.temperature,
                "num_predict": request.max_tokens
            }
        }
        
        # Ollama API í˜¸ì¶œ
        response = requests.post(
            f"{OLLAMA_HOST}/api/generate",
            json=payload,
            timeout=300
        )
        
        if response.status_code != 200:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"Ollama API ì˜¤ë¥˜: {response.text}"
            )
        
        result = response.json()
        
        return {
            "success": True,
            "response": result.get("response", ""),
            "model": MODEL_NAME,
            "prompt": request.prompt,
            "done": result.get("done", False)
        }
        
    except requests.exceptions.Timeout:
        raise HTTPException(status_code=504, detail="Ollama ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
    except requests.exceptions.ConnectionError:
        raise HTTPException(status_code=503, detail="Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")


@app.post("/api/generate/stream")
async def generate_with_image_stream(
    image: UploadFile = File(..., description="ì…ë ¥ ì´ë¯¸ì§€ íŒŒì¼"),
    prompt: str = Form(..., description="ì²˜ë¦¬í•  í”„ë¡¬í”„íŠ¸"),
    temperature: float = Form(0.7, description="ìƒì„± ì˜¨ë„ (0.0-1.0)"),
):
    """
    ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì•„ì„œ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ
    (ì‹¤ì‹œê°„ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°›ì•„ë³¼ ìˆ˜ ìˆìŒ)
    """
    try:
        from fastapi.responses import StreamingResponse
        
        # ì´ë¯¸ì§€ ì½ê¸°
        image_bytes = await image.read()
        
        # ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì¦
        if not validate_image(image_bytes):
            raise HTTPException(status_code=400, detail="ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€ í˜•ì‹ì…ë‹ˆë‹¤.")
        
        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
        image_base64 = encode_image_to_base64(image_bytes)
        
        # Ollama API ìš”ì²­ ì¤€ë¹„
        payload = {
            "model": MODEL_NAME,
            "prompt": prompt,
            "images": [image_base64],
            "stream": True,
            "options": {
                "temperature": temperature
            }
        }
        
        def generate():
            """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
            try:
                response = requests.post(
                    f"{OLLAMA_HOST}/api/generate",
                    json=payload,
                    stream=True,
                    timeout=300
                )
                
                for line in response.iter_lines():
                    if line:
                        yield line + b'\n'
            except Exception as e:
                yield json.dumps({"error": str(e)}).encode() + b'\n'
        
        return StreamingResponse(
            generate(),
            media_type="application/x-ndjson"
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", 3000))
    host = os.getenv("HOST", "0.0.0.0")
    
    print(f"ğŸš€ Ollama Qwen3-VL API Server ì‹œì‘")
    print(f"ğŸ“ ì„œë²„ ì£¼ì†Œ: http://{host}:{port}")
    print(f"ğŸ¤– ëª¨ë¸: {MODEL_NAME}")
    print(f"ğŸ”— Ollama í˜¸ìŠ¤íŠ¸: {OLLAMA_HOST}")
    print(f"ğŸ“š API ë¬¸ì„œ: http://{host}:{port}/docs")
    
    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level="info"
    )

