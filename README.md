# PDF to Summary AI - Ollama Qwen3-VL ì„œë²„

ì´ í”„ë¡œì íŠ¸ëŠ” VESSL í™˜ê²½ì—ì„œ Ollamaì™€ Qwen3-VL:235b ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” Python API ì„œë²„ì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­](#ì‹œìŠ¤í…œ-ìš”êµ¬ì‚¬í•­)
- [ì„¤ì¹˜ ë°©ë²•](#ì„¤ì¹˜-ë°©ë²•)
- [VESSL í™˜ê²½ ì„¤ì •](#vessl-í™˜ê²½-ì„¤ì •)
- [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
- [API ì—”ë“œí¬ì¸íŠ¸](#api-ì—”ë“œí¬ì¸íŠ¸)
- [í…ŒìŠ¤íŠ¸](#í…ŒìŠ¤íŠ¸)
- [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

## ğŸ–¥ï¸ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­
- **OS**: Linux (Ubuntu 20.04+ ê¶Œì¥)
- **Python**: 3.9 ì´ìƒ
- **GPU**: NVIDIA GPU (ìµœì†Œ 48GB VRAM ê¶Œì¥)
  - Qwen3-VL:235bëŠ” ë§¤ìš° í° ëª¨ë¸ë¡œ ë§ì€ VRAM í•„ìš”
  - A100 80GB ë˜ëŠ” H100 GPU ê¶Œì¥
- **ë””ìŠ¤í¬ ê³µê°„**: ìµœì†Œ 300GB ì´ìƒ (ëª¨ë¸ í¬ê¸°: ~235GB)
- **ë©”ëª¨ë¦¬**: ìµœì†Œ 64GB RAM ê¶Œì¥
- **ë„¤íŠ¸ì›Œí¬**: ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ ì•ˆì •ì ì¸ ì¸í„°ë„· ì—°ê²°

### VESSL ê¶Œì¥ ë¦¬ì†ŒìŠ¤ ì„¤ì •
```yaml
resource:
  cluster: vessl-gcp-oregon  # ë˜ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ í´ëŸ¬ìŠ¤í„°
  preset: gpu-l-mem  # ë˜ëŠ” A100/H100 ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¦¬ì…‹
```

## ğŸ“¦ ì„¤ì¹˜ ë°©ë²•

### ë¹ ë¥¸ ì‹œì‘ (ê¶Œì¥)

```bash
# ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd /Users/jhyunwoo/projects/pdf-to-summary-ai

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x *.sh

# ë¹ ë¥¸ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (ëª¨ë“  ê²ƒì„ ìë™ìœ¼ë¡œ ì„¤ì •)
./quick_start.sh

# ì„œë²„ ì‹¤í–‰
./run_server.sh
```

### ìˆ˜ë™ ì„¤ì¹˜

#### 1ë‹¨ê³„: Python ê°€ìƒí™˜ê²½ ì„¤ì •

```bash
# ê°€ìƒí™˜ê²½ ìƒì„± ë° ì˜ì¡´ì„± ì„¤ì¹˜
./setup_venv.sh

# ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ:
python3 -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

#### 2ë‹¨ê³„: Ollama ì„¤ì¹˜

```bash
# Ollama ì„¤ì¹˜
./setup_ollama.sh

# ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ:
curl -fsSL https://ollama.com/install.sh | sh
ollama --version
```

#### 3ë‹¨ê³„: Ollama ì„œë²„ ì‹œì‘

```bash
# Ollama ì„œë²„ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰)
./start_ollama.sh

# ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:11434/api/tags

# ì„œë²„ ë¡œê·¸ í™•ì¸
tail -f ollama.log
```

#### 4ë‹¨ê³„: Qwen3-VL ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼ - ì•½ 235GB)
./download_model.sh

# ë˜ëŠ” ì§ì ‘ ollama ëª…ë ¹ì–´ ì‚¬ìš©
ollama pull qwen3-vl:235b

# ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸
ollama list
```

âš ï¸ **ì£¼ì˜**: ëª¨ë¸ ë‹¤ìš´ë¡œë“œëŠ” ì¸í„°ë„· ì†ë„ì— ë”°ë¼ ìˆ˜ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### 5ë‹¨ê³„: Python API ì„œë²„ ì‹œì‘

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# ì„œë²„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© (ê¶Œì¥)
./run_server.sh

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
python server.py

# í™˜ê²½ ë³€ìˆ˜ì™€ í•¨ê»˜ ì‹œì‘
PORT=8080 OLLAMA_HOST=http://localhost:11434 python server.py

# ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰
nohup python server.py > server.log 2>&1 &
```

## ğŸ”§ VESSL í™˜ê²½ ì„¤ì •

### VESSL Runìœ¼ë¡œ ì‹¤í–‰í•˜ê¸°

VESSLì—ì„œ ì´ í”„ë¡œì íŠ¸ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:

#### 1. VESSL CLI ì„¤ì¹˜
```bash
pip install vessl
vessl configure
```

#### 2. VESSL Run ìƒì„±

`vessl.yaml` íŒŒì¼ ìƒì„±:

```yaml
name: ollama-qwen3vl-server
description: Ollama Qwen3-VL API Server

image: quay.io/vessl-ai/torch:2.0.1-cuda11.8-r15

resources:
  cluster: vessl-gcp-oregon
  preset: gpu-l-mem  # A100 ë˜ëŠ” H100 ê¶Œì¥

import:
  /code:
    git:
      url: <YOUR_GIT_REPO_URL>
      ref: main

run:
  - command: |
      set -e
      cd /code
      
      # Python ì˜ì¡´ì„± ì„¤ì¹˜
      pip install -r requirements.txt
      
      # Ollama ì„¤ì¹˜
      chmod +x setup_ollama.sh start_ollama.sh download_model.sh
      ./setup_ollama.sh
      
      # Ollama ì„œë²„ ì‹œì‘
      ./start_ollama.sh
      
      # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìë™ìœ¼ë¡œ y ì…ë ¥)
      echo "y" | ./download_model.sh
      
      # API ì„œë²„ ì‹œì‘
      python server.py

ports:
  - name: api
    type: http
    port: 8000
  - name: ollama
    type: http
    port: 11434

workdir: /code
```

#### 3. Run ì‹¤í–‰
```bash
vessl run create -f vessl.yaml
```

### í™˜ê²½ ë³€ìˆ˜

| ë³€ìˆ˜ëª… | ê¸°ë³¸ê°’ | ì„¤ëª… |
|--------|--------|------|
| `OLLAMA_HOST` | `http://localhost:11434` | Ollama ì„œë²„ ì£¼ì†Œ |
| `MODEL_NAME` | `qwen3-vl:235b` | ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ |
| `PORT` | `8000` | API ì„œë²„ í¬íŠ¸ |
| `HOST` | `0.0.0.0` | API ì„œë²„ í˜¸ìŠ¤íŠ¸ |
| `OLLAMA_MODELS` | `/workspace/.ollama/models` | ëª¨ë¸ ì €ì¥ ê²½ë¡œ |

## ğŸš€ ì‚¬ìš© ë°©ë²•

### ì¼ìƒì ì¸ ì›Œí¬í”Œë¡œìš°

```bash
# 1. í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd /Users/jhyunwoo/projects/pdf-to-summary-ai

# 2. ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# 3. ì„œë²„ ì‹œì‘
./run_server.sh

# 4. ì„œë²„ ì¢…ë£Œ (ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ)
./stop_all.sh
```

### ì„œë²„ ìƒíƒœ í™•ì¸

```bash
# ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:8000/

# í—¬ìŠ¤ì²´í¬
curl http://localhost:8000/health

# í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep -E "ollama|python server.py"
```

### API ë¬¸ì„œ í™•ì¸

ë¸Œë¼ìš°ì €ì—ì„œ ë‹¤ìŒ ì£¼ì†Œë¡œ ì ‘ì†:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

## ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸

### 1. ì´ë¯¸ì§€ + í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬

**POST** `/api/generate`

ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì•„ì„œ ëª¨ë¸ì´ ì²˜ë¦¬í•œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

**Request (multipart/form-data):**
- `image` (file): ì´ë¯¸ì§€ íŒŒì¼
- `prompt` (string): ì²˜ë¦¬í•  í”„ë¡¬í”„íŠ¸
- `temperature` (float, optional): ìƒì„± ì˜¨ë„ (0.0-1.0, ê¸°ë³¸ê°’: 0.7)
- `max_tokens` (int, optional): ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 2000)

**Example:**
```bash
curl -X POST "http://localhost:8000/api/generate" \
  -F "image=@example.jpg" \
  -F "prompt=ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ìˆë‚˜ìš”?" \
  -F "temperature=0.7" \
  -F "max_tokens=2000"
```

**Response:**
```json
{
  "success": true,
  "response": "ì´ë¯¸ì§€ì—ëŠ” ê³ ì–‘ì´ê°€ ë³´ì…ë‹ˆë‹¤...",
  "model": "qwen3-vl:235b",
  "prompt": "ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ìˆë‚˜ìš”?",
  "done": true,
  "total_duration": 5000000000,
  "eval_count": 150
}
```

### 2. í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬

**POST** `/api/generate/text`

ì´ë¯¸ì§€ ì—†ì´ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

**Request (JSON):**
```json
{
  "prompt": "í•œêµ­ì˜ ìˆ˜ë„ëŠ”?",
  "temperature": 0.7,
  "max_tokens": 2000
}
```

**Example:**
```bash
curl -X POST "http://localhost:8000/api/generate/text" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "í•œêµ­ì˜ ìˆ˜ë„ëŠ”?",
    "temperature": 0.7,
    "max_tokens": 2000
  }'
```

### 3. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

**POST** `/api/generate/stream`

ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µì„ ë°›ì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Example:**
```bash
curl -X POST "http://localhost:8000/api/generate/stream" \
  -F "image=@example.jpg" \
  -F "prompt=ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”" \
  -F "temperature=0.7"
```

### 4. í—¬ìŠ¤ì²´í¬

**GET** `/health`

ì„œë²„ì™€ Ollama ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

## ğŸ§ª í…ŒìŠ¤íŠ¸

### Python í…ŒìŠ¤íŠ¸ í´ë¼ì´ì–¸íŠ¸

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python test_client.py

# ì´ë¯¸ì§€ì™€ í•¨ê»˜ í…ŒìŠ¤íŠ¸
python test_client.py test.jpg "ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"

# ì˜ˆì œ ì‚¬ìš©ë²• ì‹¤í–‰
python example_usage.py test.jpg
```

### cURL í…ŒìŠ¤íŠ¸

```bash
# 1. ì„œë²„ ìƒíƒœ í™•ì¸
curl http://localhost:8000/health

# 2. í…ìŠ¤íŠ¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8000/api/generate/text" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
    "temperature": 0.7,
    "max_tokens": 500
  }'

# 3. ì´ë¯¸ì§€ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
curl -X POST "http://localhost:8000/api/generate" \
  -F "image=@test.jpg" \
  -F "prompt=ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”" \
  -F "temperature=0.7" \
  -F "max_tokens=1000"
```

## ğŸ›‘ ì„œë²„ ì¢…ë£Œ

```bash
# ìë™ ì¢…ë£Œ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
./stop_all.sh

# ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ ì¢…ë£Œ
pkill -f "python server.py"
pkill ollama

# í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep -E "ollama|python server.py"
```

## ğŸ” ë¬¸ì œ í•´ê²°

### ê°€ìƒí™˜ê²½ ê´€ë ¨

```bash
# ê°€ìƒí™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•Šì€ ê²½ìš°
source venv/bin/activate

# ê°€ìƒí™˜ê²½ ì¬ìƒì„±
rm -rf venv
./setup_venv.sh
```

### Ollama ì„œë²„ê°€ ì‹œì‘ë˜ì§€ ì•ŠëŠ” ê²½ìš°

```bash
# ë¡œê·¸ í™•ì¸
cat ollama.log

# Ollama í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep ollama

# í¬íŠ¸ ì‚¬ìš© í™•ì¸
lsof -i :11434

# ìˆ˜ë™ìœ¼ë¡œ ì¬ì‹œì‘
pkill ollama
sleep 2
./start_ollama.sh
```

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨

```bash
# ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
df -h

# Ollama ëª¨ë¸ ëª©ë¡ í™•ì¸
ollama list

# ëª¨ë¸ ì¬ë‹¤ìš´ë¡œë“œ
ollama rm qwen3-vl:235b
ollama pull qwen3-vl:235b
```

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

```bash
# GPU ì‚¬ìš©ëŸ‰ í™•ì¸
nvidia-smi

# ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© ê³ ë ¤
ollama pull qwen3-vl:14b  # ë” ì‘ì€ ë²„ì „
```

### API ì„œë²„ ì—°ê²° ì‹¤íŒ¨

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸
source venv/bin/activate

# Python ì„œë²„ ë¡œê·¸ í™•ì¸
cat server.log

# í¬ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•˜ì—¬ ë¡œê·¸ í™•ì¸
python server.py

# í¬íŠ¸ê°€ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸
lsof -i :8000

# ë‹¤ë¥¸ í¬íŠ¸ë¡œ ì‹œë„
PORT=8080 python server.py
```

### VESSL í™˜ê²½ì—ì„œ ë””ìŠ¤í¬ ê³µê°„ ë¬¸ì œ

VESSLì—ì„œëŠ” `/workspace` ë””ë ‰í† ë¦¬ì— ì˜êµ¬ ì €ì¥ì†Œê°€ ë§ˆìš´íŠ¸ë©ë‹ˆë‹¤:

```bash
# ëª¨ë¸ ì €ì¥ ê²½ë¡œë¥¼ /workspaceë¡œ ì„¤ì •
export OLLAMA_MODELS=/workspace/.ollama/models
mkdir -p $OLLAMA_MODELS
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### 1. GPU ë©”ëª¨ë¦¬ ìµœì í™”

```bash
# Ollama í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export OLLAMA_NUM_PARALLEL=1  # ë³‘ë ¬ ìš”ì²­ ìˆ˜ ì œí•œ
export OLLAMA_MAX_LOADED_MODELS=1  # ë¡œë“œëœ ëª¨ë¸ ìˆ˜ ì œí•œ
```

### 2. ëª¨ë¸ ì–‘ìí™” ì‚¬ìš©

ë” ì‘ì€ VRAMìœ¼ë¡œ ì‹¤í–‰í•˜ë ¤ë©´ ì–‘ìí™”ëœ ëª¨ë¸ì„ ì‚¬ìš©:

```bash
# 4-bit ì–‘ìí™” ë²„ì „ (VRAM ì‚¬ìš©ëŸ‰ ê°ì†Œ)
ollama pull qwen3-vl:235b-q4
```

### 3. ë°°ì¹˜ ì²˜ë¦¬

ì—¬ëŸ¬ ìš”ì²­ì„ ë™ì‹œì— ì²˜ë¦¬í•˜ë ¤ë©´ ì„œë²„ ì›Œì»¤ ìˆ˜ë¥¼ ì¡°ì •:

```bash
uvicorn server:app --host 0.0.0.0 --port 8000 --workers 2
```

## ğŸ› ï¸ ê°œë°œ ëª¨ë“œ

ê°œë°œ ì¤‘ì—ëŠ” ìë™ ë¦¬ë¡œë“œ í™œì„±í™”:

```bash
uvicorn server:app --host 0.0.0.0 --port 8000 --reload
```

## ğŸ“ ë¼ì´ì„¼ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„¼ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.

## ğŸ¤ ê¸°ì—¬

ì´ìŠˆì™€ PRì„ í™˜ì˜í•©ë‹ˆë‹¤!

## ğŸ“§ ë¬¸ì˜

ì§ˆë¬¸ì´ë‚˜ ë¬¸ì œê°€ ìˆìœ¼ë©´ ì´ìŠˆë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

---

**ì°¸ê³  ë§í¬:**
- [Ollama ê³µì‹ ë¬¸ì„œ](https://github.com/ollama/ollama)
- [Qwen3-VL ëª¨ë¸](https://huggingface.co/Qwen)
- [VESSL ë¬¸ì„œ](https://docs.vessl.ai/)
- [FastAPI ë¬¸ì„œ](https://fastapi.tiangolo.com/)

